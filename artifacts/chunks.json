["Title: Grounded Answering and Citations - Instruct the model: “Use ONLY the provided context; if unsure, say you don't know.” - Require bracketed citations like [1], [2] tied to retrieved chunks. - Short answers with precise quotes are often better than long, speculative text. - Post-check the answer: verify each cited chunk actually supports the claim.", "Title: BM25 Basics - BM25 ranks documents by term frequency, inverse document frequency, and document length normalization. - Tokenization quality matters: lowercase, remove stopwords, and keep alphanumerics for best results. - BM25 excels when exact terms are important (e.g., error codes, names, API keys). - Combine BM25 with dense retrieval to cover both exact and semantic matches.", "Title: FAISS Tuning Notes - Normalize embeddings for cosine-like inner product search. - For large corpora: IVF1024,HNSW32 can be a good trade-off; train IVF on a sample of vectors. - Use add_with_ids to keep stable mappings between vectors and your metadata. - For updates: keep an append-only log of new vectors and rebuild IVF/HNSW periodically.", "Title: Evaluating RAG Systems - Measure retrieval quality: recall@k, precision@k, and NDCG for ranking quality. - For answer quality, use exact match, F1, or ROUGE-L depending on the task. - Add a “no-answer” option: if context lacks evidence, the system should abstain. - Reproducible evaluation needs a fixed dataset of questions and ground-truth citations.", "Title: Hybrid Retrieval Tips - Dense search uses embeddings to match meaning; sparse search uses keywords and term statistics (e.g., BM25). - Score normalization helps merge scores from different sources. Min-max or z-score are common choices. - Reciprocal Rank Fusion (RRF) is robust: it rewards documents that rank well in any list. - Alpha controls the balance: higher alpha favors dense similarity; lower alpha favors keyword matches. - Overlapping chunks help reduce boundary issues; 500 tokens with 100 overlap is a reasonable starting point.", "Title: Vector Databases 101 - A vector database stores embeddings (numeric vectors) and supports nearest neighbor search. - Common index types include Flat (exact), IVF (inverted file for coarse quantization), and HNSW (graph-based). - Cosine similarity is often used for normalized embeddings; inner product equals cosine when vectors are L2-normalized. - FAISS is a popular library; IndexFlatIP is exact but can be slower on large corpora. IVF+PQ or HNSW improves scalability. - Recall can be improved by re-ranking the top candidates with a more precise scorer. - Hybrid retrieval combines dense semantic search with sparse keyword search like BM25."]